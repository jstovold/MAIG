{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning \n",
    "With thanks to Dr Massimiliano Patacchiola for the gridworld implementation, because I'm too lazy to implement it myself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from gridworld import GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridworld is provided alongside this notebook -- we're just re-using a simple grid environment from elsewhere. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD(0) update function is pretty simple, we just need to take our existing estimate of V and update it according to:\n",
    "\n",
    "$$\n",
    "  V_t(s) \\leftarrow V_t(s) + \\alpha \\left[ r(s) + \\gamma V_{t+1}(s) - V_t(s) \\right]\n",
    "$$\n",
    "\n",
    "In the below, our state is represented as a simple (x,y) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD(value_matrix, state, new_state, reward, alpha, gamma):\n",
    "\n",
    "    v = value_matrix[state[0], state[1]]\n",
    "    v_t1 = value_matrix[new_state[0], new_state[1]]\n",
    "    \n",
    "    value_matrix[state[0], state[1]] += alpha * (reward + gamma * v_t1 - v)\n",
    "    \n",
    "    return value_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our TD(0) update function defined, we can pull together the environment etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld(3, 4)\n",
    "\n",
    "state_matrix = np.zeros((3,4))\n",
    "state_matrix[0, 3] = 1\n",
    "\n",
    "print(\"State Matrix:\")\n",
    "print(state_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_matrix = np.zeros((3,4))\n",
    "reward_matrix[0, 3] = 1\n",
    "\n",
    "print(\"Reward Matrix:\")\n",
    "print(reward_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for probabilistic transitions:\n",
    "# transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "#                               [0.1, 0.8, 0.1, 0.0],\n",
    "#                               [0.0, 0.1, 0.8, 0.1],\n",
    "#                               [0.1, 0.0, 0.1, 0.8]])\n",
    "\n",
    "transition_matrix = np.eye(4);\n",
    "print(transition_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular environment class, we need to provide the state / reward / transition matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.setStateMatrix(state_matrix)\n",
    "env.setRewardMatrix(reward_matrix)\n",
    "env.setTransitionMatrix(transition_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start our value iteration -- we are going to pull the loop out to keep the code (relatively) clean..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_learning_loop(tot_epoch,\n",
    "                    diff_epoch, \n",
    "                    print_epoch, \n",
    "                    env, \n",
    "                    value_matrix, \n",
    "                    alpha, \n",
    "                    gamma):\n",
    "\n",
    "    epoch_i = 0;\n",
    "    diffs = np.zeros((3,4,int(tot_epoch / diff_epoch)+1))\n",
    "    prev_v = value_matrix.copy();\n",
    "    \n",
    "    for epoch in range(tot_epoch):\n",
    "        state = env.reset(exploring_starts=False)\n",
    "\n",
    "        for step_i in range(1000):\n",
    "\n",
    "            action = random.randint(0,3);\n",
    "            new_state, reward, done = env.step(action)\n",
    "            value_matrix = TD(value_matrix, state, new_state, reward, alpha, gamma)\n",
    "\n",
    "            state = new_state\n",
    "            if done: break\n",
    "\n",
    "        if(epoch % diff_epoch == 0):\n",
    "            diff = value_matrix - prev_v;\n",
    "            diffs[...,epoch_i] = diff;\n",
    "            prev_v = value_matrix.copy()\n",
    "            epoch_i += 1;\n",
    "            print(epoch)\n",
    "            \n",
    "        if (epoch % print_epoch == 0):\n",
    "            print(\"\\r\\nValue matrix after \" + str(epoch+1) + \" iterations:\")\n",
    "            print(value_matrix)\n",
    "            \n",
    "    return (value_matrix, diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "value_matrix = np.zeros((3,4))\n",
    "gamma        = 0.9\n",
    "alpha        = 0.1\n",
    "tot_epoch    = int(1e5)\n",
    "print_epoch  = int(1e4)\n",
    "diff_epoch   = int(1e3)\n",
    "\n",
    "print(\"\\r\\nValue matrix after 0 iterations:\")\n",
    "print(value_matrix)\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "(value_matrix, diffs) = TD_learning_loop(tot_epoch,\n",
    "                                         diff_epoch, \n",
    "                                         print_epoch, \n",
    "                                         env, \n",
    "                                         value_matrix, \n",
    "                                         alpha, \n",
    "                                         gamma)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"%d iterations completed in approx. %d seconds.\" % (tot_epoch, toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Value matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "print(value_matrix.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        plt.plot(diffs[i,j,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final step is to determine the optimal policy $\\pi^*$ from the computed value matrix $V$, which is trivial by following:\n",
    "\n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_a\\left[ r(s,a) + \\gamma V^*\\left( \\delta(s,a) \\right) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the Q-Learning update rule:\n",
    "\n",
    "$$\n",
    "    Q'(s, a) = (1 - \\alpha)Q'(s, a) + \\alpha\\mathopen{}\\left(r + \\gamma \\max_{a'}Q'(s', a')\\right)\\mathclose{}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn (state, action) pairs, we need a data structure which provides all permutations of (s, a). This is the Q table, which gives us all possible states against all possible actions. We can then learn the Q value for all (s, a) pairs, and store it in the Q table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((12, 4));\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the same environment as with TD Learning, we need to 'unroll' the state representation into a single integer (so we can index the Q table). This function doesn't need to be complicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll(state, rows = 3, cols = 4):\n",
    "    # default: 4col, 3row environment\n",
    "    # (x, y) -> i\n",
    "    \n",
    "    x = state[1];\n",
    "    y = state[0];\n",
    "    cell_loc = cols * y + x;\n",
    "\n",
    "    return cell_loc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start the learning process. The code below is the same as with TD Learning, except we have replaced the TD function with the Q learning update function from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_loop(tot_epoch, \n",
    "                    diff_epoch, \n",
    "                    print_epoch, \n",
    "                    env, \n",
    "                    q_table, \n",
    "                    alpha, \n",
    "                    alpha_dt,\n",
    "                    gamma):\n",
    "    epoch_i = 0;\n",
    "    diffs = np.zeros((12, 4, int(tot_epoch / diff_epoch) + 1));\n",
    "    prev_q = q_table.copy();\n",
    "\n",
    "    for epoch in range(tot_epoch):\n",
    "        state = env.reset(exploring_starts=False)\n",
    "\n",
    "        for step_i in range(1000):\n",
    "            # Take an action:\n",
    "            action = random.randint(0,3);\n",
    "            new_state, reward, done = env.step(action);\n",
    "            new_state_r = unroll(new_state);\n",
    "            \n",
    "            # Update Q:\n",
    "            q_table[state, action] = (1 - alpha)*q_table[state, action] + \\\n",
    "                                         alpha * (reward + gamma * max(q_table[new_state_r,:]));\n",
    "            state = new_state_r;\n",
    "\n",
    "            if done: break\n",
    "\n",
    "        if(epoch % diff_epoch == 0):\n",
    "#             print(epoch)\n",
    "            diff = np.subtract(q_table, prev_q);\n",
    "            diffs[...,epoch_i] = diff;\n",
    "            prev_q = q_table.copy();\n",
    "            epoch_i += 1\n",
    "            \n",
    "        if (epoch % print_epoch == 0):\n",
    "            print(\"\\r\\nQ table after \" + str(epoch+1) + \" iterations:\")\n",
    "            print(q_table.round(2))\n",
    "            \n",
    "        alpha = alpha - alpha_dt;\n",
    "        \n",
    "    return (q_table, diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we have to do is initialise our parameters / variables and throw them at the `q_learning_loop()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "alpha_0  = 0.001\n",
    "alpha_dt = 0\n",
    "\n",
    "gamma = 0.1\n",
    "tot_epoch = int(1e5)\n",
    "print_epoch = int(1e4)\n",
    "diff_epoch = int(1e3)\n",
    "q_table = np.zeros((12, 4));\n",
    "\n",
    "print(\"\\r\\nQ table after 0 iterations:\")\n",
    "print(q_table.round(2))\n",
    "\n",
    "(q_table, diffs)  = q_learning_loop(tot_epoch, \n",
    "                                    diff_epoch, \n",
    "                                    print_epoch, \n",
    "                                    env,\n",
    "                                    q_table, \n",
    "                                    alpha_0,\n",
    "                                    alpha_dt,\n",
    "                                    gamma)\n",
    "\n",
    "\n",
    "toc = time.time()\n",
    "print(\"%d iterations completed in %d seconds.\" % (tot_epoch, toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can plot the changes in our Q table to show the convergence over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(12):\n",
    "    for j in range(4):\n",
    "        plt.plot(diffs[i,j,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
